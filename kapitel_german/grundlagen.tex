\newpage
\section{Theoretische Grundlagen}\label{sec:grundlagen}
% Designprinzipien, KI-Grundlagen, Multimodalität
\subsection{Minimalistisches Logodesign}\label{subsec:minimalistisches_logodesign}
\subsubsection{Prinzipien und Kriterien}\label{subsubsec:prinzipien_und_kriterien}

Die Designphilosophie des Minimalismus lässt sich wissenschaftlich mit dem Prinzip von Ockhams Rasiermesser (Ockham's Razor) begründen. Dieses besagt, dass bei funktional gleichwertigen Alternativen die einfachste Lösung zu bevorzugen ist, da unnötige Elemente die Effizienz verringern und die Wahrscheinlichkeit von Fehlern oder unvorhergesehenen Problemen erhöhen \parencite[172]{Lidwell2010}. Im Kontext des Logodesigns zielt der Minimalismus darauf ab, eine Markenidentität auf ihre wesentlichsten Elemente zu reduzieren, um maximale Klarheit, Wiedererkennbarkeit und Funktionalität zu erreichen. Ein minimalistisches Logo eliminiert überflüssige Details, Verzierungen und komplexe Strukturen zugunsten einfacher Formen, klarer Linien und einer begrenzten Farbpalette \parencite[52]{Wheeler2017}. Diese Reduktion ist kein Selbstzweck, sondern dient der Destillation der Kernbotschaft einer Marke in ein prägnantes visuelles Zeichen.

Die wissenschaftliche Grundlage für die Wirksamkeit minimalistischer Gestaltungen findet sich unter anderem in den Gestaltgesetzen der Wahrnehmungspsychologie. \textcite[144]{Lidwell2010} erklären mit Prinzipien wie dem Gesetz der Prägnanz (die Tendenz, mehrdeutige Formen als möglichst einfache, stabile Figuren wahrzunehmen), warum einfache Logos schneller erfasst und besser erinnert werden. Ein minimalistisches Logo nutzt diese kognitiven Mechanismen, um eine unmittelbare und intuitive Verbindung zum Betrachter herzustellen.

Die zentralen Kriterien für ein erfolgreiches minimalistisches Logo lassen sich wie folgt zusammenfassen:

\subparagraph*{Einfachheit und Klarheit}\label{para:einfachheit_und_klarheit} Das oberste Prinzip ist die Reduktion auf das Wesentliche. Jedes Element muss eine klare Funktion haben; alles, was nicht zur Kernaussage beiträgt, wird entfernt. Laut \textcite[52]{Wheeler2017} sorgt diese Klarheit dafür, dass das Logo auch bei flüchtiger Betrachtung sofort verständlich ist und seine Botschaft ohne kognitive Hürden transportiert.

\subparagraph*{Einprägsamkeit}\label{para:einpraegsamkeit} Durch die Reduktion auf eine einzigartige, einfache Form wird die Einprägsamkeit signifikant erhöht. Komplexe Designs überfordern das visuelle Gedächtnis, während ein klares, unverwechselbares Zeichen leichter gespeichert und wiedererkannt wird. Nach \textcite[2]{hjalmarsson2021impact} ist dies in einem übersättigten Marktumfeld von entscheidender Bedeutung.

\subparagraph*{Zeitlosigkeit}\label{para:zeitlosigkeit} Minimalistische Logos vermeiden modische Trends, die schnell veralten können. Durch den Fokus auf universelle Formen, klassische Typografie und eine durchdachte Farbgebung streben sie eine lange Lebensdauer an. Ein zeitloses Logo muss nicht ständig modernisiert werden und trägt so zur Stabilität der Markenidentität bei \parencite[40]{Wheeler2017}.

\subparagraph*{Vielseitigkeit und Skalierbarkeit}\label{para:vielseitigkeit_und_skalierbarkeit} Ein Logo muss in unterschiedlichsten Kontexten und Größen funktionieren – vom winzigen Favicon auf einer Website bis zur großflächigen Plakatwand. Minimalistische Designs sind hier klar im Vorteil: Ihre einfachen Formen und klaren Linien bleiben auch bei starker Verkleinerung oder in monochromer Darstellung (z. B. als Gravur) stabil und erkennbar \parencite[44]{Wheeler2017}.


Zusammenfassend lässt sich sagen, dass minimalistisches Logodesign eine strategische Disziplin ist, die auf psychologischen Prinzipien der Wahrnehmung und kognitiven Verarbeitung beruht. Es geht nicht um einen Mangel an Kreativität, sondern um die Kunst, Komplexität zu reduzieren und eine Botschaft mit maximaler Effizienz und Eleganz zu vermitteln.


\subsubsection{Die Topologie der Marken: Klassifikation minimalistischer Logotypen}\label{subsubsec:die_topologie_der_marken_klassifikation_minimalistischer_logotypen}

Die Definition und Klassifikation von Logos variiert in der Literatur \parencite[51]{Wheeler2017}\parencite{rashgraphic2024}. Um eine klare und prägnante Übersicht zu bieten, konzentriert sich die folgende Typologie auf die stilistische Einordnung minimalistischer Logos. Die Wahl des Stils hängt strategisch von der Art der Marke und der zu vermittelnden Botschaft ab. Die folgende Klassifikation, die zur Ordnung der unterschiedlichen Relationen zwischen dem visuellen Zeichen und der darunter liegenden Markenidentität dient, lehnt sich stark an das von \textcite[51]{Wheeler2017} vorgeschlagene Rahmenwerk an. Übergeordnet lassen sich drei Hauptkategorien unterscheiden: typografische Logos, Bildmarken und kombinierte Marken.

\paragraph*{Typografische Logos}\label{para:typografische_logos}
Bei typografischen Logos fungiert der Markenname selbst oder dessen Initialen als Logo \parencite[68]{Lupton2010}. Diese Kategorie umfasst sowohl Wortmarken, die den vollständigen Namen nutzen (z. B. $\textit{Google}$ Abb. \ref{fig:google_logo}), als auch Buchstabenformen (Monogramm), die auf einzelnen Buchstaben oder Initialen basieren (z. B. $\textit{IBM}$) \parencite[51]{Wheeler2017}. Die minimalistische Wirkung wird durch die Wahl einer klaren, oft serifenlosen Schriftart, die präzise Anordnung der Buchstaben und den bewussten Einsatz von Leerraum erzeugt. Ihre Stärke liegt in der direkten Benennung und hohen Wiedererkennbarkeit.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.25\textwidth]{abbildungen/google-logo.png}
	\caption{Beispiel für Wortmarken: Google-Logo, Quelle: \parencite{CloudflareGoogleLogo}}
	\label{fig:google_logo}
\end{figure}

\paragraph*{Bildmarken}\label{para:bildmarken}
Bildmarken nutzen Symbole oder Bilder, um eine Marke zu repräsentieren. \textcite[51]{Wheeler2017} unterteilt sie in drei Hauptkategorien, eine Einteilung, die auch in anderer Quelle zu finden ist \parencite{rashgraphic2024}:

\subparagraph*{Embleme (Emblems)}\label{subpara:embleme_emblems} Ein Emblem-Logo kombiniert Text und Bild zu einer einzigen zusammenhängenden Einheit. Der Text ist dabei meist in ein Symbol oder Icon eingeschlossen, was ein integriertes visuelles Element schafft und oft ein traditionelles, klassisches Erscheinungsbild erzeugt. Ein bekanntes Beispiel ist das Logo von $\textit{Volkswagen}$ \parencite{rashgraphic2024}.

\subparagraph*{Piktoriale Marken (Pictorial Marks)}\label{subpara:piktorale_marken_pictorial_marks} Diese stellen ein wiedererkennbares, literales Abbild dar, das stark vereinfacht und stilisiert wurde \parencite[51]{Wheeler2017}. Ihre Wirksamkeit beruht auf der schnellen Assoziation mit einem realweltlichen Objekt. Das Bild ist entweder direkt mit dem Produkt verbunden (z. B. der Vogel von $\textit{Twitter}$) oder wird, wie das ikonische $\textit{Apple}$-Logo (Abb. \ref{fig:apple_logo}), durch stetige Präsenz zur Metapher.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.25\textwidth]{abbildungen/apple-logo.png}
	\caption{Beispiel für piktoriale Logos: Apple-Logo, Quelle: \parencite{StickPNGAppleLogo}}
	\label{fig:apple_logo}
\end{figure}

\subparagraph*{Abstrakte Marken}\label{subpara:abstrakte_marken} Als reinste Form minimalistischer Gestaltung nutzen sie eine geometrische oder abstrakte Form, die eine tiefere Idee vermittelt, ohne ein reales Objekt darzustellen \parencite[51]{Wheeler2017}. Die Verbindung zur Marke wird durch Konsistenz aufgebaut und lässt durch strategische Ambiguität Interpretationsspielraum. Beispiele sind der $\textit{Nike}$ Swoosh oder das Zielscheiben-Symbol von $\textit{Target}$.



\paragraph*{Kombinierte Marken}\label{para:kombinierte_marken}
Die kombinierte Marke ist eine Verbindung aus Wort- und Bildmarke zu einem einzigen Logo \parencite{rashgraphic2024}. Text und Bild werden kombiniert, um die Markenbotschaft zu verstärken und zu verdeutlichen, wofür ein Unternehmen steht. Man unterscheidet zwischen integrierten und eigenständigen kombinierten Marken, was diesen Ansatz sehr flexibel und populär macht. Bekannte Beispiele sind $\textit{PayPal}$ (Abb. \ref{fig:paypal_logo}), $\textit{Spotify}$ oder $\textit{Rolex}$.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.25\textwidth]{abbildungen/paypal-logo.png}
	\caption{Beispiel für kombinierte Marken: PayPal-Logo, Quelle: \parencite{WikipediaPaypalLogo2014}}
	\label{fig:paypal_logo}
\end{figure}




\subsection{Generative \acs{KI}-Modelle}\label{subsec:generative_ac_ki-modelle}

Generative \ac{KI} bezeichnet eine Klasse von Algorithmen, die in der Lage sind, neue, originäre Daten zu erzeugen, die in ihrer Charakteristik den Trainingsdaten ähneln. Diese Modelle lernen die zugrundeliegende Verteilung eines Datensatzes und können daraus neue Instanzen synthetisieren, seien es Bilder, Texte, Musik oder andere komplexe Datenformate \parencite{GOODFELLOW2014}. Die jüngsten Fortschritte in diesem Bereich, insbesondere bei Diffusionsmodellen, haben die kreativen Möglichkeiten der KI revolutioniert und bilden die technologische Grundlage für diese Arbeit.

\subsubsection{Von \acp{GAN} zu Diffusionsmodellen: Eine paradigmatische Verschiebung}\label{subsubsec:von_gans_zu_diffusionsmodellen_eine_paradigmatische_verschiebung}

\textcite{GOODFELLOW2014} prägten mit der Einführung von \acp{GAN} im Jahr 2014 maßgeblich die Ära moderner generativer Bildsynthese. Ein \ac{GAN} besteht aus zwei konkurrierenden neuronalen Netzen: einem Generator, der versucht, realistische Daten zu erzeugen, und einem Diskriminator, der versucht, echte von gefälschten Daten zu unterscheiden. Durch diesen adversariellen Prozess lernt der Generator schrittweise, immer überzeugendere Bilder zu synthetisieren. Obwohl \acp{GAN} beeindruckende Ergebnisse erzielten, litten sie oft unter Trainingsinstabilitäten, Modenkollaps (Mode Collapse), bei dem der Generator nur eine begrenzte Vielfalt an Ausgaben produziert, und einer schwierigen Konvergenz \parencite[2-3]{arjovsky2017wasserstein}.

Als Antwort auf diese Herausforderungen führten \textcite[3]{HO2020} ab 2020 die \acp{DDPM} als neues, stabileres Paradigma ein. Diffusionsmodelle sind probabilistische Modelle, die darauf trainiert sind, einen schrittweisen Entrauschungsprozess zu erlernen. Der Prozess besteht aus zwei Teilen:

\subparagraph*{Forward Process (Noising)}\label{para:forward_process_noising} In diesem festen, nicht-gelernten Prozess wird ein Bild aus den Trainingsdaten schrittweise über eine definierte Anzahl von Zeitschritten $T$ mit Gaußschem Rauschen überlagert. Am Ende dieses Prozesses, bei $t=T$, ist das ursprüngliche Bild $x_0$ in reines isotropes Rauschen $x_T$ umgewandelt.

\subparagraph*{Reverse Process (Denoising)}\label{para:reverse_process_denoising} Ein neuronales Netz, typischerweise eine \acs{UNet}-Architektur (\acl{UNet}) \parencite[5]{HO2020}, wird darauf trainiert, diesen Prozess umzukehren. Für jeden Zeitschritt $t$ lernt das Modell, das im vorherigen Schritt hinzugefügte Rauschen vorherzusagen und zu entfernen. Indem es bei reinem Rauschen startet und diesen Entrauschungsschritt $T$-mal wiederholt, kann das Modell ein neues, sauberes Bild synthetisieren. Dieser Ansatz ist von Natur aus stabiler im Training und erzeugt qualitativ hochwertigere und vielfältigere Ergebnisse als viele \ac{GAN}-Architekturen \parencite{dhariwal2021diffusion}\inlinecomment{keine seitenzahl hier ok}.


\subsubsection{Latent Diffusion: Effizienz durch Kompression}\label{subsubsec:latent_diffusion_effizienz_durch_kompression}

Eine entscheidende Weiterentwicklung der Diffusionsmodelle sind die von \textcite{ROMBACH2022} vorgestellten \acp{LDM}, die die rechenintensive Diffusion nicht im hochdimensionalen Pixelraum, sondern in einem komprimierten, latenten Raum durchführen. Ein vortrainierter \ac{VAE} übersetzt das Bild in eine kompakte latente Repräsentation. Der rechenintensive Diffusions- und Entrauschungsprozess findet in diesem kleineren Raum statt, bevor der Decoder des \ac{VAE} das entrauschte latente Bild zurück in den Pixelraum transformiert. Dieser Ansatz, der Modellen wie Stable Diffusion zugrunde liegt, reduziert den Rechenaufwand drastisch und ermöglicht die Generierung hochauflösender Bilder auf handelsüblicher Hardware.


\subsection{Konditionierte Modellierung: Bild+Text Repräsentationen}\label{subsec:konditionierte_modellierung_bild+text_repraesentationen}
Während generative Modelle wie \acp{LDM} in der Lage sind, qualitativ hochwertige Bilder zu erzeugen, benötigen sie für eine gezielte Anwendung eine präzise Steuerung. Die konditionierte Modellierung schlägt hier die Brücke, indem sie verschiedene Arten von Informationen (Modalitäten) – typischerweise Text und Bild – miteinander verknüpft \parencite{ZHANG2023}\parencite{RAMESH2022}. Dies ermöglicht es, den Generierungsprozess durch semantische und strukturelle Vorgaben zu konditionieren.

\subsubsection{\acs{CLIP}: Die Brücke zwischen Bild und Text}\label{subsubsec:clip_die_bruecke_zwischen_bild_und_text}

Die Fähigkeit, die Bildgenerierung durch Text zu steuern, ist ein zentraler Durchbruch für die praktische Anwendung generativer Modelle. Das Fundament dafür legten \textcite{radford2021learning} mit dem von OpenAI entwickelten Modell \ac{CLIP}. \ac{CLIP} wird auf riesigen Datensätzen von Bild-Text-Paaren aus dem Internet trainiert und lernt, eine Verbindung zwischen visuellen und textuellen Konzepten herzustellen.

Es besteht aus zwei Hauptkomponenten: einem Bild-Encoder (z. B. ein \ac{ViT}) und einem Text-Encoder (z. B. ein Transformer). Das Trainingsziel ist es, die Repräsentationen eines korrespondierenden Bild-Text-Paares in einem gemeinsamen, hochdimensionalen Vektorraum (Embedding Space) möglichst nahe beieinander zu platzieren, während die Repräsentationen von nicht-passenden Paaren weit voneinander entfernt positioniert werden (Contrastive Learning).

Das Ergebnis ist ein multimodaler semantischer Raum, in dem der Vektor für das Bild eines Hundes dem Vektor für den Text ``ein Bild von einem Hund'' sehr ähnlich ist. Diese Eigenschaft macht \ac{CLIP} zu einem mächtigen Werkzeug, um die semantische Übereinstimmung zwischen einem Text-Prompt und einem generierten Bild zu bewerten (\acs{CLIP} Score) \parencite{Hessel2021CLIPScoreAR} oder um den Generierungsprozess von Diffusionsmodellen direkt zu steuern (\acs{CLIP} Guidance) \parencite[15]{RAMESH2022}. Indem der Entrauschungsprozess in eine Richtung gelenkt wird, die die \ac{CLIP}-Ähnlichkeit zum gewünschten Text maximiert, können Modelle wie DALL-E 2 und Stable Diffusion, wie von \textcite{RAMESH2022} und \textcite[3]{ROMBACH2022} gezeigt, Bilder erzeugen, die präzise den textuellen Vorgaben entsprechen.


\subsubsection{ControlNet: Präzise strukturelle Kontrolle}\label{subsubsec:controlnet_praezise_strukturelle_kontrolle}

Während die textuelle Steuerung durch \ac{CLIP} die semantische Ausrichtung vorgibt, fehlte lange eine präzise Kontrolle über die strukturelle Komposition des generierten Bildes, wie z. B. die Pose einer Figur, die Anordnung von Objekten oder die Einhaltung einer vorgegebenen Form. Diese Lücke schließt die von \textcite{ZHANG2023} vorgestellte ControlNet-Architektur, die eine zusätzliche Ebene der Konditionierung einführt, um die räumliche Anordnung im Bild exakt zu steuern.

Die Kernidee von ControlNet ist ebenso einfach wie genial: Anstatt ein riesiges Diffusionsmodell von Grund auf neu zu trainieren, um strukturelle Vorgaben zu lernen, wird das bereits vorhandene, auf Milliarden von Bildern trainierte Wissen des Modells (z. B. Stable Diffusion) unangetastet gelassen. Stattdessen wird eine parallele, trainierbare Kopie der Encoder-Blöcke des Basismodells erstellt. Diese Kopie ist das eigentliche ``ControlNet'' \parencite[2]{ZHANG2023}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{abbildungen/controlnet_eingabe_moeglichkeiten.png}
	\caption{Kontrollvarianten zur konditionierten Bildgenerierung mit ControlNet, Quelle: \parencite[Abb. 7]{ZHANG2023}}
	\label{fig:controlnet_eingabe_moeglichkeiten}
\end{figure}

Der Prozess lässt sich in drei Schritten erklären:
\begin{enumerate}
	\item \textbf{Erhalt des Basismodells:} Die Gewichte des ursprünglichen, vortrainierten Diffusionsmodells werden eingefroren (``locked''). Dadurch bleibt dessen Fähigkeit, hochwertige und vielfältige Bilder zu generieren, vollständig erhalten.
	\item \textbf{Hinzufügen der Kontrolle:} Hier liegt der entscheidende Unterschied. Während die Architektur und die initialen Gewichte der Encoder-Blöcke kopiert werden, wird die Eingabeverarbeitung des ControlNet erweitert. Es erhält nicht nur die verrauschte Bildeingabe aus dem vorigen Schritt, sondern zusätzlich eine Control Map (z. B. ein Kantenbild oder eine Skizze wie in Abb. \ref{fig:controlnet_eingabe_moeglichkeiten}). Diese beiden Informationen werden kombiniert und gemeinsam verarbeitet. Das ControlNet lernt somit, die strukturellen Vorgaben aus der Control Map zu interpretieren und mit der Bildinformation zu verknüpfen.
	\item \textbf{Behutsame Integration:} Die Ausgabe des ControlNet wird nicht direkt mit dem Basismodell vermischt. Stattdessen wird sie über spezielle ``Zero Convolution''-Layer zu den entsprechenden Blöcken des ursprünglichen Diffusionsmodells addiert. Diese Layer sind zu Beginn des Trainings so initialisiert, dass sie ein Null-Signal ausgeben. Das stellt sicher, dass das ControlNet anfangs keinen Einfluss hat und das Basismodell nicht durch ``schädliches'' Rauschen gestört wird. Während des Trainings lernt das ControlNet schrittweise, sinnvolle Anpassungen zu erzeugen, die den Generierungsprozess des Basismodells in die durch die Control Map vorgegebene Richtung lenken.
\end{enumerate}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{abbildungen/controlnet_scribble_example.png}
	\caption{Beispielhafte Bildgenerierung mit Kantenbild und Prompt zur Konditionierung, Quelle: \parencite[Abb. 1]{ZHANG2023}}
	\label{fig:controlnet_scribble_example}
\end{figure}

Im Ergebnis zwingt das ControlNet das Diffusionsmodell dazu, sich exakt an die vorgegebene Struktur zu halten, während die Details (Textur, Farbe, Stil) weiterhin durch den Text-Prompt bestimmt werden (Abb. \ref{fig:controlnet_scribble_example}). Dieser Ansatz ist extrem leistungsfähig, da er die generalisierte Fähigkeit großer Modelle mit präziser, aufgabenspezifischer Kontrolle kombiniert, ohne dass das Basismodell neu trainiert werden muss. Für die Logo-Generierung bedeutet dies, dass eine einfache Skizze die Form und Komposition des Logos exakt vorgeben kann, während der Text-Prompt den Stil (``minimalistisch'', ``flach'') und den Inhalt (``ein roter Apfel'') definiert.


\subsection{Fine-Tuning von Foundation-Models}\label{subsec:finetuning_foundation_models}
\subsubsection*{Die Herausforderung}

Moderne generative Modelle, wie z. B. Stable Diffusion, sind sogenannte Foundation-Models. Sie wurden auf riesigen, allgemeinen Datensätzen trainiert und besitzen ein breites Verständnis von Sprache und visuellen Konzepten \parencite[2]{zhang2025parameter}. Um sie für spezifische Aufgaben oder Domänen (z. B. die Erzeugung von Logos in einem bestimmten Stil) zu spezialisieren, ist ein Fine-Tuning erforderlich.

Das traditionelle Full Fine-Tuning, bei dem alle Milliarden von Parametern des Modells aktualisiert werden, bringt jedoch, wie \textcite[3]{Zhihao_2025} aufzeigen, erhebliche Nachteile mit sich:

\begin{itemize}
	\item \textbf{Hoher Rechenaufwand:} Das Training erfordert immense \acs{GPU}-Ressourcen und Zeit.
	\item \textbf{Großer Speicherbedarf:} Für jede spezialisierte Aufgabe muss eine vollständige Kopie des Modells gespeichert werden, was mehrere Gigabyte pro Version bedeutet.
	\item \textbf{Katastrophales Vergessen:} Das Modell kann während der Spezialisierung sein ursprüngliches, breites Wissen ``vergessen'', was seine Allgemeinfähigkeit beeinträchtigt.
\end{itemize}

Diese Herausforderungen haben zur Entwicklung von \acs{PEFT}-Methoden geführt.

\subsubsection*{Parameter-Effizientes Fine-Tuning (\acs{PEFT})}\label{subsubsec:parameter-effizientes_fine-tuning_acpeft}

\acs{PEFT}-Methoden verfolgen das von \textcite{zhang2025parameter} beschriebene Ziel, ein Modell mit minimalem Rechen- und Speicheraufwand anzupassen, indem nur eine kleine Anzahl von Parametern trainiert wird, während der Großteil des ursprünglichen Modells eingefroren bleibt. Dies löst die oben genannten Probleme, wofür es, wie \textcite[4]{Zhihao_2025} zeigen, verschiedene Strategien zur Umsetzung gibt:

\subparagraph*{Adapter-Module}\label{subpara:adapter-module} Hierbei werden kleine, zusätzliche neuronale Netzwerkschichten (``Adapter'') zwischen die bestehenden Ebenen des Foundation-Models eingefügt. Während des Fine-Tunings werden ausschließlich die Parameter dieser Adapter trainiert. Dies ist effektiv, kann aber die Inferenzlatenz leicht erhöhen, da zusätzliche Berechnungsschritte erforderlich sind.

\subparagraph*{Prompt-Tuning und Prefix-Tuning}\label{subpara:prompt-tuning_und_prefix-tuning} Diese Methoden modifizieren nicht die Gewichte des Modells, sondern optimieren stattdessen einen kleinen, trainierbaren Vektor (einen ``soft prompt'' oder ``prefix''), der der Eingabe des Modells vorangestellt wird. Das Modell lernt, seine Ausgabe basierend auf diesem optimierten Kontextvektor zu konditionieren.

\subparagraph*{Low-Rank Adaptation (\acs{LoRA})}\label{subpara:low-rank_adaptation_lora} Die von \textcite[4]{HU2021} eingeführte Methode ist besonders populär und relevant für die aktuelle Forschung. Sie basiert auf der Hypothese, dass die Gewichtsänderungen ($\Delta W$), die für die Anpassung eines Modells erforderlich sind, eine niedrige ``intrinsische Dimensionalität'' (low rank) haben. Anstatt die gesamte Gewichtsmatrix $W$ (Dimension $d \times k$) zu aktualisieren, approximiert \acs{LoRA} die Änderung $\Delta W$ durch das Produkt zweier kleinerer, niedrigrangiger Matrizen: $\Delta W = B \cdot A$, wobei $A$ die Dimension $d \times r$ und $B$ die Dimension $r \times k$ hat \parencite[4]{Zhihao_2025}. Der Rang $r$ ist dabei ein kleiner Hyperparameter ($r \ll \min(d, k)$).

\begin{formel}[H]
	\caption{Gewichtsaktualisierung mittels Low-Rank Adaptation (\acs{LoRA}) \parencite{HU2021}}
	\label{formel:lora_update}
	\begin{equation}
		W_{neu} = W + \Delta W = W + B \cdot A
		\label{eq:lora_update}
	\end{equation}
\end{formel}

Während des Trainings werden nur die Matrizen $A$ und $B$ optimiert. Dies reduziert die Anzahl der trainierbaren Parameter drastisch. Ein wesentlicher Vorteil von \acs{LoRA} ist, dass zur Inferenzzeit die trainierte Matrix $B \cdot A$ berechnet und direkt zu $W$ addiert werden kann. Somit entsteht keine zusätzliche Inferenzlatenz, im Gegensatz zu Adapter-Modulen.


\subsection{Bewertungsmetriken und Qualitätskriterien}\label{subsec:bewertungsmetriken_und_qualitaetskriterien}
Die objektive Bewertung der von generativen Modellen erzeugten Bilder ist eine komplexe Herausforderung, da sie sowohl semantische Korrektheit als auch visuelle Qualität umfassen muss. In der Forschung haben sich mehrere Metriken etabliert, die unterschiedliche Aspekte der Bildqualität quantifizieren. Für die Evaluation der Logo-Generierung in dieser Arbeit werden drei zentrale Metriken verwendet: der \ac{CLIP}-Score zur Messung der Text-Bild-Übereinstimmung, der \ac{SSIM} (\acl{SSIM}) zur Bewertung der strukturellen Ähnlichkeit und die \ac{FID} (\acl{FID}) zur Beurteilung der allgemeinen Bildqualität und Realitätstreue.

\subsubsection{\acs{CLIP}-Score: Semantische Übereinstimmung}\label{subsubsec:clip-score_semantische_uebereinstimmung}
Der von \textcite{Hessel2021CLIPScoreAR} vorgestellte \ac{CLIP}-Score misst die semantische Ähnlichkeit zwischen einem generierten Bild und dem zugehörigen Text-Prompt. Er nutzt das von OpenAI entwickelte \ac{CLIP}-Modell, das darauf trainiert wurde, Text- und Bildrepräsentationen in einem gemeinsamen multimodalen Vektorraum zu verorten \parencite{radford2021learning}. Die Kernidee besteht darin, die Kosinus-Ähnlichkeit zwischen dem Vektor des generierten Bildes ($v_{\text{img}}$) und dem Vektor des Text-Prompts ($v_{\text{text}}$) zu berechnen.

Mathematisch wird der Score wie folgt ermittelt:
1.  Ein Bild-Encoder und ein Text-Encoder transformieren die jeweiligen Eingaben in Feature-Vektoren.
2.  Diese Vektoren werden normalisiert: $\hat{v} = v / \lVert v \rVert_2$.
3.  Die Kosinus-Ähnlichkeit $s = \hat{v}_{\text{img}}^\top \hat{v}_{\text{text}}$ wird berechnet, was einem Wert im Bereich von [-1, 1] entspricht.

Ein höherer \ac{CLIP}-Score indiziert eine bessere semantische Übereinstimmung zwischen Bild und Text. Diese Metrik ist besonders wertvoll, um zu überprüfen, ob das Modell die inhaltlichen Vorgaben des Prompts korrekt umgesetzt hat. Sie ist robust gegenüber stilistischen Variationen, solange die Kernkonzepte erhalten bleiben.

\subsubsection{\ac{SSIM}: Strukturelle Ähnlichkeit}\label{subsubsec:acssim_strukturelle_aehnlichkeit}
Der \ac{SSIM} ist eine von \textcite{ssim_original} eingeführte Metrik zur Messung der strukturellen Ähnlichkeit zwischen zwei Bildern. Im Kontext der konditionierten Logo-Generierung wird diese Metrik verwendet, um die Übereinstimmung zwischen der Eingabeskizze (Referenzbild) und dem generierten Logo (Ausgabebild) zu quantifizieren. Anstatt pixelbasierter Fehler wie beim \ac{MSE} zu messen, vergleicht \ac{SSIM}, wie von \textcite[3]{snell2017learning} beschrieben, zwei Bildausschnitte, $x$ und $y$, anhand von drei Komponenten: Luminanz (I), Kontrast (C) und Struktur (S).
Da die Referenz in dieser Arbeit eine kontrast- und farbarme Skizze ist, liegt der Fokus der Messung primär auf der Strukturkomponente (S), die bewertet, wie gut die Form und Anordnung von Linien im generierten Bild mit der Vorlage übereinstimmen. Die Luminanz- und Kontrast-Komponenten sind in diesem Anwendungsfall von untergeordneter Bedeutung.

Die drei Vergleichsfunktionen sind in Formel \ref{formel:ssim_components} definiert:
\begin{formel}[H]
	\caption{Komponenten des \ac{SSIM} \parencite[3]{snell2017learning}}
	\label{formel:ssim_components}
	\begin{align}
		I(x,y) & = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}             \\
		C(x,y) & = \frac{2\sigma_x\sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \\
		S(x,y) & = \frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3}
	\end{align}
\end{formel}
Hierbei bezeichnen $\mu_x$ und $\mu_y$ die mittlere Pixelintensität, während $\sigma_x$ und $\sigma_y$ die Standardabweichungen in einem lokalen Bildfenster um $x$ oder $y$ darstellen. $\sigma_{xy}$ ist die Kovarianz zwischen den beiden Fenstern. Die Konstanten $C_1, C_2$ und $C_3$ dienen der numerischen Stabilität.

Der finale \ac{SSIM}-Wert ist in Formel \ref{formel:ssim} definiert:
\begin{formel}[H]
	\caption{\ac{SSIM} \parencite[3]{snell2017learning}}
	\label{formel:ssim}
	\begin{equation}
		\text{SSIM}(x,y) = I(x,y) \cdot C(x,y) \cdot S(x,y)
	\end{equation}
\end{formel}
Der Gesamtwert für das Bild ist der Mittelwert über alle lokalen Fenster. Der Wertebereich liegt zwischen -1 und 1, wobei 1 eine perfekte strukturelle Übereinstimmung bedeutet (Abb. \ref{fig:ssim_beispiel}).
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{abbildungen/ssim_beispiel.png}
	\caption{Vergleich von \ac{SSIM} und \ac{MSE} anhand eines Beispiels, Quelle: \parencite{ScikitImageSSIM}}
	\label{fig:ssim_beispiel}
\end{figure}
Ein hoher \ac{SSIM}-Wert zeigt an, dass das Modell die Form und das Layout der Skizze exakt eingehalten hat, was für die Logo-Generierung ein entscheidendes Qualitätsmerkmal ist.

\subsubsection{\ac{FID}: Bildqualität und Realismus}\label{subsubsec:acfid_bildqualitaet_und_realismus}
Die \ac{FID} ist eine von \textcite[6]{heusel2017gans} eingeführte, weit verbreitete Metrik zur Bewertung der Qualität und Diversität von generierten Bildern. Sie misst den Abstand zwischen der Verteilung von Merkmalen (Features) realer Bilder und der Verteilung von Merkmalen generierter Bilder und korreliert dabei mit der menschlichen Wahrnehmung von Bildqualität \parencite[11]{heusel2017gans}. Diese Features werden aus einer tiefen Schicht eines vortrainierten Inception-v3-Netzwerks extrahiert.

Die Berechnung erfolgt in drei Schritten:
1.  Extraktion von Inception-Features für einen Satz realer Bilder und einen Satz generierter Bilder.
2.  Modellierung der beiden Feature-Verteilungen als multivariate Gaußverteilungen mit Mittelwert ($m_r, m_g$) und Kovarianzmatrix ($C_r, C_g$).
3.  Berechnung der Fréchet-Distanz zwischen diesen beiden Verteilungen (Formel \ref{formel:fid}):
\begin{formel}[H]
	\caption{Fréchet Inception Distance (FID) \parencite{heusel2017gans}}
	\label{formel:fid}
	\begin{equation}
		\text{FID} = \lVert m_r - m_g \rVert_2^2 + \operatorname{Tr}\Big(C_r + C_g - 2 (C_r C_g)^{1/2}\Big)
	\end{equation}
\end{formel}
Ein niedrigerer \ac{FID}-Wert bedeutet, dass die generierten Bilder in ihrer statistischen Verteilung den realen Bildern ähnlicher sind, was auf eine höhere visuelle Qualität, weniger Artefakte und eine größere Vielfalt hindeutet. Im Gegensatz zu \ac{CLIP}-Score und \ac{SSIM}, die spezifische Konditionierungen prüfen, bewertet \ac{FID} die allgemeine ``Realitätsnähe'' der Modellausgaben.

Zusammenfassend lässt sich sagen, dass diese drei Metriken ein komplementäres Bild der Modellleistung liefern: Ein gutes Modell sollte semantisch korrekt (hoher \ac{CLIP}-Score), strukturell treu (hoher \ac{SSIM}) und visuell hochwertig (niedriger \ac{FID}) sein.
