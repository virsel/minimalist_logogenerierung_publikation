\begin{abstract}
	This paper explores the resource-efficient fine-tuning of a Stable Diffusion model for the generation of minimalist logos. By leveraging a multimodal approach that combines text prompts with structural sketches via ControlNet, we address the limitations of standard text-to-image models in adhering to specific design constraints. We fine-tune the model on a curated dataset of 1,500 minimalist logos using consumer-grade hardware. A core contribution of this work is a systematic analysis of Low-Rank Adaptation (LoRA) hyperparameters, identifying optimal configurations for stylistic coherence and structural fidelity. We present a quantitative evaluation using CLIP, FID, and SSIM metrics, alongside qualitative case studies, demonstrating that high-quality, controllable design automation is achievable with limited computational resources.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Minimalist logo design relies on reduction, clarity, and structural precision - qualities that are often challenging for general purpose generative models to achieve consistently. While large-scale text-to-image models excel at artistic composition, they frequently struggle to produce the clean, vector-like aesthetics required for professional branding or to strictly adhere to a user's layout constraints \parencite[1]{bertao2023blind}.

This work presents a specialized pipeline for generating minimalist logos by fine-tuning Stable Diffusion v1.5 \parencite{HuggingFace_StableDiffusionv15_ModelCard}. To ensure both semantic relevance and structural control, we employ a hybrid conditioning strategy: text prompts define the style and content, while ControlNet \cite{ZHANG2023}\parencite[8]{RAMESH2022}\parencite[5]{NICHOL2021} enforces the geometric structure based on input sketches. Unlike approaches requiring massive datasets and industrial compute clusters, we focus on resource efficiency. We demonstrate that a compact, high-quality dataset of 1,500 examples is sufficient to adapt the model to the minimalist domain using consumer-grade hardware (NVIDIA RTX 5080).

Our research focuses on the optimization of the fine-tuning process itself. We conduct a rigorous analysis of Hyperparameters within the Low-Rank Adaptation (LoRA) technique, specifically examining the impact of learning rates and rank dimensions on the trade-off between training stability and generation quality.

The evaluation is primarily quantitative, utilizing the Fr√©chet Inception Distance (FID) to assess image quality, the CLIP score for semantic alignment, and the Structural Similarity Index (SSIM) to measure fidelity to the input sketches. We complement these metrics with qualitative case studies that illustrate the model's capability to translate rough sketches into polished, minimalist designs. This approach validates that accessible hardware and efficient training strategies can yield professional-grade design automation tools.
