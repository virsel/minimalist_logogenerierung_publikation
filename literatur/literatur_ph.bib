% online
%% einleitung
@online{99designsMinimalismusLogoDesign,
  author  = {{99designs}},
  title   = {Minimalismus im Logo-Design},
  year    = {2023},
  url     = {https://99designs.de/blog/logo-und-branding/minimalismus-im-logo-design/},
  urldate = {2025-08-01}
}
@online{Google_Gemini25Pro,
  author  = {{Gemini 2.5 Pro}},
  title   = {Minimalismus-Score Python-Funktion},
  year    = {2025},
  note    = {[Großes Sprachmodell]},
  url     = {https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro},
  urldate = {2025-09-23}
}

@online{lllyasvielControlNet,
  author  = {{Zhang, Lvmin}},
  title   = {ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models},
  year    = {2023},
  url     = {https://huggingface.co/lllyasviel/ControlNet},
  urldate = {2025-10-10},
  note    = {Official ControlNet Repository auf Hugging Face}
}
@online{GitHub,
  author = {{GitHub}},
  title = {{GitHub}},
  url = {https://github.com/},
  urldate = {2025-11-20}, % Das Zugriffsdatum im Format YYYY-MM-TT
  year = {2024},         % Das Jahr der Website-Erstellung oder des Zugriffs
}
@online{GoogleCloudRun,
  author  = {{Google Cloud}},
  title   = {Cloud Run: Container-Anwendungs-Hosting},
  year    = {2025},
  url     = {https://cloud.google.com/run?hl=de},
  urldate = {2025-10-22}
}
@online{ControlNetGitHub,
  author  = {Zhang, Lvmin},
  title   = {ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models},
  howpublished = {GitHub Repository},
  url     = {https://github.com/lllyasviel/ControlNet?tab=readme-ov-file},
  year    = {2023},
  urldate = {2025-10-10}
}
@online{verifiedmarketreports_grafikdesign2025,
  author   = {{Verified Market Reports}},
  title    = {Globale Grafikdesign-Marktgröße nach Endbenutzer, nach Anwendung, nach Komponenten, nach Branchenvertikat},
  note     = {Berichts-ID: 531734, Studienzeitraum: 2023-2033},
  url      = {https://www.verifiedmarketreports.com/de/product/graphic-design-market/},
  urldate  = {2025-07-28},
  year     = {2025},
  month    = {2},
  abstract = {Der Markt für Grafikdesign ist ein dynamischer Sektor, der eine breite Palette von Diensten umfasst, um visuelle Inhalte für die effektive Kommunikation von Nachrichten zu erstellen. Die Marktgröße wurde im Jahr 2024 mit 45,67 Milliarden USD bewertet und wird voraussichtlich in einem CAGR von 5,5\% von 2026 bis 2033 wachsen und USD 70,24 Milliarden bis 2033 erreichen.}
}
@online{arxiv2025,
  author   = {{Cornell University}},
  title    = {arXiv.org e-Print archive},
  note     = {Open-access preprint server for physics, math, computer science, and more},
  url      = {https://arxiv.org},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {arXiv ist ein freier und offener Preprint-Server für wissenschaftliche Arbeiten in den Bereichen Physik, Mathematik, Informatik, Statistik, Elektrotechnik, Systemwissenschaften und Ökonomie.}
}
@online{ieeexplore2025,
  author   = {{IEEE}},
  title    = {IEEE Xplore Digital Library},
  note     = {Access to scientific and technical content published by the IEEE and its publishing partners},
  url      = {https://ieeexplore.ieee.org},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {IEEE Xplore bietet Zugang zu wissenschaftlichen Artikeln, Konferenzberichten und Standards aus Technik und Informatik, veröffentlicht vom IEEE.}
}
@online{acm2025,
  author   = {{Association for Computing Machinery (ACM)}},
  title    = {ACM Digital Library},
  note     = {Comprehensive collection of full-text articles and bibliographic records covering computing and information technology},
  url      = {https://dl.acm.org},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {Die ACM Digital Library ist ein umfangreiches Archiv für Publikationen und Konferenzen aus dem Bereich Informatik und Informationstechnologie.}
}
@online{googlescholar2025,
  author   = {{Google LLC}},
  title    = {Google Scholar},
  note     = {Search engine for scholarly literature across many disciplines and sources},
  url      = {https://scholar.google.com},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {Google Scholar ist eine akademische Suchmaschine für wissenschaftliche Literatur aus verschiedenen Disziplinen und Quellen.}
}
@online{researchgate2025,
  author   = {{ResearchGate GmbH}},
  title    = {ResearchGate},
  note     = {Social network and publication repository for researchers and scientists},
  url      = {https://www.researchgate.net},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {ResearchGate ist ein soziales Netzwerk für Forschende, das den Austausch wissenschaftlicher Publikationen und Kooperationen ermöglicht.}
}
@online{onnx_ai,
  author  = {{ONNX Community}},
  title   = {ONNX GitHub Repository},
  year    = {2025},
  url     = {https://github.com/onnx/onnx},
  urldate = {2025-10-22}
}
@online{HuggingFace_StableDiffusionv15_ModelCard,
  author   = {{Hugging Face} and {runwayml} and {StabilityAI}},
  title    = {{Stable Diffusion v1-5 Model Card}},
  note     = {Latent text-to-image diffusion model},
  url      = {https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5},
  urldate  = {2025-10-06},
  year     = {2022},
  abstract = {Stable Diffusion v1-5 is a latent text-to-image diffusion model capable of generating photo-realistic images. It was fine-tuned on the Stable-Diffusion-v1-2 checkpoint for 595k steps at 512x512 resolution on "laion-aesthetics v2 5+".}
}
% inferenzbeschleunigung mit NVIDIA TensorRT
@online{NVIDIA_TensorRT_BestPractices,
  author  = {{NVIDIA Corporation}},
  title   = {Best Practices For TensorRT Performance},
  url     = {https://docs.nvidia.com/deeplearning/tensorrt/10.13.0/performance/best-practices.html#optimizing-layer-performance},
  version = {8.0.3},
  urldate = {2025-10-22}, % Datum der Abfrage/Prüfung
  organization = {NVIDIA Developer},
  note    = {Archivierte Dokumentation}
}
% inferenzbeschleunigung mit NVIDIA triton server
@online{NVIDIA_Triton_UserGuide,
  author  = {{NVIDIA Corporation}},
  title   = {NVIDIA Triton Inference Server: User Guide},
  url     = {https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html},
  urldate = {2025-10-22}, % Datum der Abfrage/Prüfung
  organization = {NVIDIA Developer},
  note    = {Aktuelle Dokumentation}
}
@online{Google_Gemini25FlashImage,
  author   = {{Google LLC}},
  title    = {{Gemini 2.5 Flash Image (Nano Banana)}},
  note     = {Modelldokumentation in Google AI Studio},
  subtitle = {{Unlock multimodal creativity for the next generation of visual apps}},
  url      = {https://aistudio.google.com/models/gemini-2-5-flash-image},
  urldate  = {2025-10-06},
  year     = {2025},
  abstract = {Vorstellung des multimodal nutzbaren Modells Gemini 2.5 Flash Image, das für visuelle Anwendungen der nächsten Generation konzipiert ist.}
}
@online{kaggle2025,
  author   = {{Kaggle Inc.}},
  title    = {Kaggle: Your Machine Learning and Data Science Community},
  note     = {Plattform für Datenwettbewerbe, Datasets und kollaboratives Machine Learning},
  url      = {https://www.kaggle.com},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {Kaggle ist eine Online-Community für Datenwissenschaftler und Machine-Learning-Entwickler. Die Plattform bietet Wettbewerbe, öffentliche Datasets, Code-Notebooks und ein aktives Forum zur Zusammenarbeit und Weiterbildung.}
}
@online{huggingface2025,
  author   = {{Hugging Face Inc.}},
  title    = {Hugging Face: The AI community building the future},
  note     = {Plattform für offene KI-Modelle, Datasets und Tools im Bereich NLP, CV und mehr},
  url      = {https://huggingface.co},
  urldate  = {2025-07-29},
  year     = {2025},
  month    = {7},
  abstract = {Hugging Face ist eine Plattform für den Austausch, das Hosting und die Entwicklung von offenen KI-Modellen, insbesondere im Bereich Natural Language Processing. Sie bietet eine große Modellbibliothek, Datasets und APIs zur einfachen Integration.}
}

@online{pytorch,
  author  = {{PyTorch Core Team}},
  title   = {PyTorch},
  year    = {2024},
  url     = {https://pytorch.org/},
  urldate = {2025-10-10}
}
@online{torchvision,
  author  = {{PyTorch Core Team}},
  title   = {TorchVision},
  year    = {2024},
  url     = {https://pytorch.org/vision/stable/index.html},
  urldate = {2025-10-10}
}
% in online quellen
@online{huggingface_diffusers,
  author  = {{Hugging Face Inc.}},
  title   = {Hugging Face Diffusers},
  year    = {2024},
  url     = {https://huggingface.co/docs/diffusers},
  urldate = {2025-10-10}
}
% in online quellen
@online{huggingface_transformers,
  author  = {{Hugging Face Inc.}},
  title   = {Hugging Face Transformers},
  year    = {2024},
  url     = {https://huggingface.co/docs/transformers},
  urldate = {2025-10-10}
}
% in online quellen
@online{huggingface_peft,
  author  = {{Hugging Face Inc.}},
  title   = {PEFT: Parameter-Efficient Fine-Tuning},
  year    = {2024},
  url     = {https://huggingface.co/docs/peft},
  urldate = {2025-10-10}
}
@online{mlflow,
  author  = {{MLflow Team}},
  title   = {MLflow},
  year    = {2024},
  url     = {https://mlflow.org/},
  urldate = {2025-10-10}
}
@online{docker,
  author  = {{Docker Inc.}},
  title   = {Docker},
  year    = {2024},
  url     = {https://www.docker.com/},
  urldate = {2025-10-10}
}
% in online quellen
@online{openai_clip,
  author  = {{OpenAI}},
  title   = {CLIP: Connecting Text and Images},
  year    = {2021},
  url     = {https://github.com/openai/CLIP},
  urldate = {2025-10-10}
}
% in online quellen
@online{scikit-image,
  author  = {{scikit-image Team}},
  title   = {scikit-image},
  year    = {2024},
  url     = {https://scikit-image.org/},
  urldate = {2025-10-10}
}
% in online quellen
@online{clean-fid,
  author  = {Parrish, GaParmar},
  title   = {clean-fid},
  year    = {2021},
  url     = {https://github.com/GaParmar/clean-fid},
  urldate = {2025-10-10}
}

@online{wen2023torchcompile,
  author  = {Wen, William},
  title   = {Introduction to torch.compile},
  year    = {2023},
  url     = {https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html},
  urldate = {2025-10-21}
}
% 2500 steps reichen für gutes training
@online{cloneofsimo_lora,
  author  = {{Cloneofsimo (GitHub Author)}},
  title   = {lora: Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning},
  year    = {2023},
  url     = {https://github.com/cloneofsimo/lora},
  urldate = {2025-10-22}
}

% logo typen
@online{rashgraphic2024,
  author   = {{RASH GRAPHIC}},
  title    = {7 Types of LOGO},
  note     = {Artikel auf Medium über die verschiedenen Arten von Logo-Designs, inkl. Emblem Logo},
  url      = {https://medium.com/@rashgraphicbd/7-types-of-logo-c69496b7735d},
  urldate  = {2025-09-26},
  year     = {2024},
  month    = {2},
  abstract = {Eine kurze Übersicht über 7 gängige Logo-Typen wie Emblem, Monogramm, Wortmarke, Bildmarke und weitere, mit Erklärungen und Anwendungsbeispielen.}
}
# datensatz
@online{iamkaikai_amazing_logos_v4,
  author       = {iamkaikai},
  title        = {{amazing\_logos\_v4} dataset},
  howpublished = {Hugging Face Datasets},
  url          = {https://huggingface.co/datasets/iamkaikai/amazing_logos_v4},
  version      = {v4},
  year         = {2024}, % oder das Jahr, in dem die v4 veröffentlicht wurde (laut Screenshot Jan 2024)
  urldate  = {2025-09-01},
}
%%%%%%%%%%%%%%%%%%%%% images
@online{CloudflareGoogleLogo,
  author  = {{Cloudflare}},
  title   = {{Technology Partner: Google}},
  url     = {https://www.cloudflare.com/de-de/partners/technology-partners/google/},
  urldate = {2025-09-29}
}
@online{StickPNGAppleLogo,
  author  = {{StickPNG}},
  title   = {{Apple Logo Grey}},
  url     = {https://www.stickpng.com/img/icons-logos-emojis/iconic-brands/apple-logo-grey},
  urldate = {2025-09-29}
}
@online{WikipediaPaypalLogo2014,
  author  = {{Wikimedia Commons}},
  title   = {{Datei:Paypal 2014 logo.png}},
  url     = {https://de.wikipedia.org/wiki/Datei:Paypal_2014_logo.png},
  urldate = {2025-09-29}
}
@online{ScikitImageSSIM,
  author  = {{scikit-image Developers}},
  title   = {{Structural similarity index}},
  url     = {https://scikit-image.org/docs/stable/auto_examples/transform/plot_ssim.html},
  urldate = {2025-10-02}
}
@online{leiner2025soscisurvey,
  author  = {Leiner, D. J.},
  title   = {SoSci Survey},
  version = {3.8.00},
  year    = {2025},
  url     = {https://www.soscisurvey.de},
  urldate = {2025-11-06},
  note    = {Software}
}
@online{DeepMind_Research,
author  = {Google DeepMind},
  title = {Research},
  organization = {DeepMind},
  url = {https://deepmind.google/research/},
  urldate = {2025-11-19},
  note = {Übersichtsseite zu Forschung und Durchbrüchen}
}
% x logo: https://pngimg.com/uploads/x_logo/small/x_logo_PNG3.png



% artikel usw (alles außer online)
%% einleitung
% einfluss von digitalisierung auf logos
@mastersthesis{hjalmarsson2021impact,
  title    = {{The impact of digitalization on logo design}},
  author   = {Hjalmarsson, Marcus and Skoglund, William},
  year     = {2021},
  school   = {Jönköping University, Jönköping International Business School},
  url      = {https://www.diva-portal.org/smash/get/diva2:1573306/FULLTEXT01.pdf},
  abstract = {Der Bericht untersucht, wie die Digitalisierung das Logo-Design verändert hat. Die Forschung zeigt, dass Design eine Wissenschaft ist - erfolgreiche Logos folgen spezifischen Richtlinien. Durch die Digitalisierung fokussieren sich Logos mehr auf Bildmarken, verwenden prägnante Farben und müssen einzigartig sowie responsiv sein, um in übersättigten digitalen Märkten zu funktionieren. Die Analyse erfolgreicher Logos (Apple, Adidas, Nike) und Interviews mit Designern von Marken wie Burger King und Netflix bestätigen diese Erkenntnisse für effektives Logo-Design im digitalen Zeitalter.}
}
% gradient accumulation
@misc{lamypoirier2021layeredgradientaccumulationmodular,
      title={Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models}, 
      author={Joel Lamy-Poirier},
      year={2021},
      eprint={2106.02679},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.02679}, 
}
% synthetic data generation for computer vision
@techreport{Gangadharaiah2024,
  type      = {Working Paper},
  author    = {Gangadharaiah, Dhanush},
  title     = {Synthetic Data Generation for Enhanced Computer Vision applications: A CAD model and Blender approach},
  institution = {Hochschule Ravensburg-Weingarten},
  pages     = {7},
  year      = {2024},
  abstract  = {In today's AI-driven era, computer vision, including autonomous driving, robotics, and healthcare, is prevalent. How-ever, acquiring ample data while managing resources and privacy constraints is challenging. This article proposes a solution: synthetic data generation. We use CAD software to craft intricate 3D models, process them in Blender, and evaluate quality using metrics like Structural Similarity and PSNR (Peak Signal to Noise Ratio). Synthetic data achieves up to 90\% similarity with real data and an average PSNR of 21dB. Our method offers a streamlined, dependable ap-proach for enhancing computer vision, especially in object detection, addressing data acquisition challenges.},
  subject      = {Image Processing},
  language  = {en}
}
% survey deep learning based image quality assessment
@article{YANG20231000,
title = {Deep Learning Based Image Quality Assessment: A Survey},
journal = {Procedia Computer Science},
volume = {221},
pages = {1000-1005},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008384},
author = {Jie Yang and Mengjin Lyu and Zhiquan Qi and Yong Shi},
keywords = {Image quality assessment, deep learning, review},
abstract = {Image quality assessment (IQA) is the problem of measuring the perceptual quality of images, which is crucial for many image-related applications. It is a difficult task due to the coupling of various degradation and the scarcity of annotations. To facilitate a better understanding of IQA, we survey the recent advances in deep learning based IQA methods, which have demonstrated remarkable performance and innovation in this field. We classify the IQA methods into two main groups: reference-based and reference-free methods. Reference-based methods compare query images with reference images, while reference-free methods do not. We further subdivide reference-based methods into full-reference and reduced-reference methods, depending on the amount of information they need from the reference images, and reference-free methods into single-input, pair-input, and multimodal-input methods, according to the form of input they use. The advantages and limitations of each category are analyzed and some representative examples of state-of-the-art methods are provided. We conclude our paper by highlighting some of the future directions and open challenges in deep learning based IQA.}
}
% tensor cores beschleunigen inferenz
@inproceedings{Ma_2025, series={ASPDAC ’25},
   title={Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores},
   url={http://dx.doi.org/10.1145/3658617.3697668},
   DOI={10.1145/3658617.3697668},
   booktitle={Proceedings of the 30th Asia and South Pacific Design Automation Conference},
   publisher={ACM},
   author={Ma, Shaobo and Fang, Chao and Shao, Haikuo and Wang, Zhongfeng},
   year={2025},
   month=jan, pages={1181–1187},
   collection={ASPDAC ’25} 
}
@misc{ruiz2023dreamboothfinetuningtexttoimage,
      title={DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation}, 
      author={Nataniel Ruiz and Yuanzhen Li and Varun Jampani and Yael Pritch and Michael Rubinstein and Kfir Aberman},
      year={2023},
      eprint={2208.12242},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2208.12242}, 
}
% ssim original
@article{ssim_original,
author = {Wang, Zhou and Bovik, Alan and Sheikh, Hamid and Simoncelli, Eero},
year = {2004},
month = {05},
pages = {600 - 612},
title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
volume = {13},
journal = {Image Processing, IEEE Transactions on},
doi = {10.1109/TIP.2003.819861}
}
@misc{podell2023sdxlimprovinglatentdiffusion,
      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, 
      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},
      year={2023},
      eprint={2307.01952},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.01952}, 
}

% imagen diffusion modell
@misc{saharia2022photorealistictexttoimagediffusionmodels,
      title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding}, 
      author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. Sara Mahdavi and Rapha Gontijo Lopes and Tim Salimans and Jonathan Ho and David J Fleet and Mohammad Norouzi},
      year={2022},
      eprint={2205.11487},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2205.11487}, 
}

% logo deisgnprinzipien mit ai aktuell schwierig umsetzbar
@article{bertao2023blind,
  title     = {A blind spot in AI-powered logo makers: visual design principles},
  author    = {Bertão, Renato Antonio and Yeoun, Myeong-Heum and Joo, Jaewoo},
  journal   = {Visual Communication},
  volume    = {24},
  number    = {1},
  pages     = {},
  year      = {2023},
  publisher = {SAGE Publications},
  doi       = {10.1177/14703572231155593},
  url       = {https://doi.org/10.1177/14703572231155593},
  abstract  = {Die Studie untersucht KI-basierte Logo-Generatoren und deren Fähigkeit, qualitativ hochwertige Logos zu erstellen. Obwohl KI Design-Aufgaben automatisieren kann, fehlt ihr die Kreativität für echte Designlösungen. Die Forscher analysierten, ob diese Tools grundlegende Logo-Design-Prinzipien wie Proportion, Balance und Einheitlichkeit umsetzen können. Logo-Design-Experten bewerteten die KI-generierten Ergebnisse negativ - die aktuellen Algorithmen produzieren hauptsächlich zufällige Layouts ohne die visuellen Eigenschaften, die ein professionelles Logo benötigt. Die Studie zeigt, dass KI-Tools ihre Algorithmen erheblich verbessern müssen, um Design-Standards zu erfüllen.}
}
%
@article{gualortí2025future,
  title     = {The Future of Logo Design: Considering Generative AI-Assisted Designs},
  author    = {Gual-Ortí, Jaume and Martínez-Moya, Joaquin A. and Amat-Cózar, Javier and Felip-Miralles, Francisco},
  journal   = {The International Journal of Visual Design},
  volume    = {19},
  number    = {1},
  pages     = {109--123},
  year      = {2025},
  publisher = {Common Ground Research Networks},
  doi       = {10.18848/2325-1581/CGP/v19i01/109-123},
  url       = {https://doi.org/10.18848/2325-1581/CGP/v19i01/109-123},
  issn      = {2325-1581},
  abstract  = {Die Studie untersuchte den Einsatz von KI im generativen Design bei der Logo-Erstellung mit Industriedesign-Studenten. Zwei praktische Übungen wurden durchgeführt - eine mit traditionellen Methoden, eine mit KI-Unterstützung. Die Ergebnisse zeigen, dass manuelle Designs mehr Markenidentität und emotionale Bindung ermöglichen, während KI-unterstützte Designs schnell viele professionell aussehende Varianten generieren. Studenten identifizierten KI-Vorteile wie Zeitoptimierung und Ideengenerierung, aber auch Nachteile wie reduzierte Kontrolle und schwierige Bearbeitung. Die Forschung schlussfolgert, dass KI zwar weniger Persönlichkeit und emotionale Tiefe bietet, aber traditionelle Methoden strategisch ergänzen kann. Hybride Ansätze, die menschliche Kreativität mit KI-Effizienz kombinieren, werden als optimal vorgeschlagen. Die Studie betont die Notwendigkeit, KI-Tools in die Design-Ausbildung zu integrieren.}
}
% konitioniertes diffusion
@article{ZHANG2023,
  author     = {Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
  title      = {Adding Conditional Control to Text-to-Image Diffusion Models},
  journal    = {CoRR},
  volume     = {abs/2302.05543},
  year       = {2023},
  url        = {https://arxiv.org/abs/2302.05543},
  eprinttype = {arXiv},
  eprint     = {2302.05543},
  timestamp  = {Thu, 16 Feb 2023 15:45:07 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2302-05543.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
% finetuning unet mit einfrieren vom rest
@misc{tehraninasab2025pixelspressureexploringfinetuning,
      title={Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging}, 
      author={Zahra TehraniNasab and Amar Kumar and Tal Arbel},
      year={2025},
      eprint={2508.14931},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2508.14931}, 
}
%% kapitel 2
@article{tdsp_article,
  title     = {AI lifecycle models need to be revised: An exploratory study in Fintech},
  author    = {Haakman, Mark and Cruz, Lu{\'\i}s and Huijgens, Hennie and van Deursen, Arie},
  journal   = {Empirical Software Engineering},
  volume    = {26},
  number    = {95},
  pages     = {1--29},
  year      = {2021},
  publisher = {Springer},
  doi       = {10.1007/s10664-021-09993-1},
  url       = {https://doi.org/10.1007/s10664-021-09993-1},
  issn      = {1573-7616},
  abstract  = {Die Studie untersucht, wie KI-Systeme in der Praxis entwickelt werden, am Beispiel einer Fallstudie bei ING. Es zeigt sich, dass bestehende Lebenszyklusmodelle wichtige Phasen wie Datensammlung, Machbarkeit und Modellüberwachung vernachlässigen. Die Herausforderungen beim Einsatz von Machine Learning liegen weniger in den Algorithmen als im gesamten Entwicklungsprozess.}
}



@article{GOODFELLOW2014,
  author     = {Ian Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  title      = {Generative Adversarial Nets},
  journal    = {CoRR},
  volume     = {abs/1406.2661},
  year       = {2014},
  url        = {http://arxiv.org/abs/1406.2661},
  eprinttype = {arXiv},
  eprint     = {1406.2661},
  timestamp  = {Mon, 23 Sep 2019 14:02:18 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/GoodfellowPMXWOCB14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

% qlora finetuning unet with lora (only attention layers or also mlp?)
@misc{dettmers2023qloraefficientfinetuningquantized,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}


@article{HO2020,
  author     = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
  title      = {Denoising Diffusion Probabilistic Models},
  journal    = {CoRR},
  volume     = {abs/2006.11239},
  year       = {2020},
  url        = {https://arxiv.org/abs/2006.11239},
  eprinttype = {arXiv},
  eprint     = {2006.11239},
  timestamp  = {Thu, 02 Jul 2020 15:37:34 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2006-11239.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

% lora training instabil
@misc{luo2024privacypreservinglowrankadaptationmembership,
      title={Privacy-Preserving Low-Rank Adaptation against Membership Inference Attacks for Latent Diffusion Models}, 
      author={Zihao Luo and Xilie Xu and Feng Liu and Yun Sing Koh and Di Wang and Jingfeng Zhang},
      year={2024},
      eprint={2402.11989},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.11989}, 
}

% lora training instabil 2
@misc{hayou2024impactinitializationlorafinetuning,
      title={The Impact of Initialization on LoRA Finetuning Dynamics}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2406.08447},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.08447}, 
}

% vlm für detailreiche image captions
@inproceedings{zeng-etal-2025-enhancing-large,
    title = "Enhancing Large Vision-Language Models with Ultra-Detailed Image Caption Generation",
    author = "Zeng, Yu  and
      Qi, Yukun  and
      Zhao, Yiming  and
      Bao, Xikun  and
      Chen, Lin  and
      Chen, Zehui  and
      Huang, Shiting  and
      Zhao, Jie  and
      Zhao, Feng",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1357/",
    doi = "10.18653/v1/2025.emnlp-main.1357",
    pages = "26703--26729",
    ISBN = "979-8-89176-332-6",
    abstract = "High-quality image captions are essential for improving modality alignment and visual understanding in Large Vision-Language Models (LVLMs). However, the scarcity of ultra-detailed image caption data limits further advancements. This paper presents a systematic pipeline for generating high-quality, ultra-detailed image captions, encompassing both pre-processing and post-processing stages. In the pre-processing stage, we classify and deduplicate images, extract visual information using expert tools, and leverage GPT-4o with structured prompts to generate initial captions. To enhance comprehensiveness, we introduce an expansion strategy based on Large Language Models (LLMs), defining eight descriptive dimensions to refine and extend captions, which serve as seed data for training a proprietary captioner model. In the post-processing stage, we incorporate human error-correction annotations and an active learning-inspired approach to refine low-quality samples. Using high-quality corrected data, we apply Direct Preference Optimization (DPO) and develop a critic-rewrite pipeline, training a sentence-level critic model to mitigate hallucinations. Experimental results demonstrate that our ultra-detailed captions significantly enhance LVLMs' perception and cognitive abilities across multiple vision-language benchmarks. The code and dataset are available at https://github.com/yuzeng0-0/UltraCaption."
}

% vllm for logo captioning
@misc{bordes2024introductionvisionlanguagemodeling,
      title={An Introduction to Vision-Language Modeling}, 
      author={Florian Bordes and Richard Yuanzhe Pang and Anurag Ajay and Alexander C. Li and Adrien Bardes and Suzanne Petryk and Oscar Mañas and Zhiqiu Lin and Anas Mahmoud and Bargav Jayaraman and Mark Ibrahim and Melissa Hall and Yunyang Xiong and Jonathan Lebensold and Candace Ross and Srihari Jayakumar and Chuan Guo and Diane Bouchacourt and Haider Al-Tahan and Karthik Padthe and Vasu Sharma and Hu Xu and Xiaoqing Ellen Tan and Megan Richards and Samuel Lavoie and Pietro Astolfi and Reyhane Askari Hemmat and Jun Chen and Kushal Tirumala and Rim Assouel and Mazda Moayeri and Arjang Talattof and Kamalika Chaudhuri and Zechun Liu and Xilun Chen and Quentin Garrido and Karen Ullrich and Aishwarya Agrawal and Kate Saenko and Asli Celikyilmaz and Vikas Chandra},
      year={2024},
      eprint={2405.17247},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17247}, 
}

@book{zar2010,
  author = {Zar, Jerrold H.},
  title = {Biostatistical Analysis},
  edition = {5th},
  publisher = {Prentice-Hall/Pearson},
  address = {Upper Saddle River, N.J.},
  year = {2010}
}

@article{ROMBACH2022,
  author     = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj{\"{o}}rn Ommer},
  title      = {High-Resolution Image Synthesis with Latent Diffusion Models},
  journal    = {CoRR},
  volume     = {abs/2112.10752},
  year       = {2022},
  url        = {https://arxiv.org/abs/2112.10752},
  eprinttype = {arXiv},
  eprint     = {2112.10752},
  timestamp  = {Thu, 06 Jan 2022 17:09:47 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2112-10752.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{HU2021,
  author     = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Han and Weizhu Chen and Lu Wang},
  title      = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal    = {CoRR},
  volume     = {abs/2106.09685},
  year       = {2021},
  url        = {https://arxiv.org/abs/2106.09685},
  eprinttype = {arXiv},
  eprint     = {2106.09685},
  timestamp  = {Wed, 14 Jul 2021 16:32:02 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{FOMLeitfaden2024,
  author = {FOM Hochschule},
  title = {Leitfaden zur Gestaltung wissenschaftlicher Arbeiten},
  edition = {Version 1.4},
  month = {3},
  year = {2024}
}

@article{RAMESH2022,
  author     = {Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  title      = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  journal    = {CoRR},
  volume     = {abs/2204.06125},
  year       = {2022},
  url        = {https://arxiv.org/abs/2204.06125},
  eprinttype = {arXiv},
  eprint     = {2204.06125},
  timestamp  = {Thu, 14 Apr 2022 17:21:40 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2204-06125.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Hessel2021CLIPScoreAR,
  title   = {CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
  author  = {Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Le Bras and Yejin Choi},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2104.08718},
  url     = {https://api.semanticscholar.org/CorpusID:233296711}
}

% fid score für evaluierung wurde hier eingeführt
@article{heusel2017gans,
  title   = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  author  = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {30},
  year    = {2017}
}

% metrik mse und ssim für training und evaluation
@inproceedings{snell2017learning,
  title     = {Learning to Generate Images With Perceptual Similarity Metrics},
  author    = {Snell, Jake and Swersky, Kevin and Zemel, Richard S},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {1112--1121},
  year      = {2017}
}

@article{NICHOL2021,
  author     = {Alex Nichol and Prafulla Dhariwal and Aditya Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and Ilya Sutskever and Mark Chen},
  title      = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  journal    = {CoRR},
  volume     = {abs/2112.10741},
  year       = {2021},
  url        = {https://arxiv.org/abs/2112.10741},
  eprinttype = {arXiv},
  eprint     = {2112.10741},
  timestamp  = {Thu, 06 Jan 2022 17:09:47 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2112-10741.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{VINYALS2015,
  author     = {Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
  title      = {Show and Tell: A Neural Image Caption Generator},
  journal    = {CoRR},
  volume     = {abs/1411.4555},
  year       = {2015},
  url        = {http://arxiv.org/abs/1411.4555},
  eprinttype = {arXiv},
  eprint     = {1411.4555},
  timestamp  = {Mon, 13 Jun 2016 16:35:10 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/VinyalsTEB14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{clip4captioning,
  author     = {Ghandi, Taraneh and Pourreza, Hamidreza and Mahyar, Hamidreza},
  title      = {Deep Learning Approaches on Image Captioning: A Review},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3617592},
  doi        = {10.1145/3617592},
  abstract   = {Image captioning is a research area of immense importance, aiming to generate natural language descriptions for visual content in the form of still images. The advent of deep learning and more recently vision-language pre-training techniques has revolutionized the field, leading to more sophisticated methods and improved performance. In this survey article, we provide a structured review of deep learning methods in image captioning by presenting a comprehensive taxonomy and discussing each method category in detail. Additionally, we examine the datasets commonly employed in image captioning research, as well as the evaluation metrics used to assess the performance of different captioning models. We address the challenges faced in this field by emphasizing issues such as object hallucination, missing context, illumination conditions, contextual understanding, and referring expressions. We rank different deep learning methods’ performance according to widely used evaluation metrics, giving insight into the current state-of-the-art. Furthermore, we identify several potential future directions for research in this area, which include tackling the information misalignment problem between image and text modalities, mitigating dataset bias, incorporating vision-language pre-training methods to enhance caption generation, and developing improved evaluation tools to accurately measure the quality of image captions.},
  journal    = {ACM Comput. Surv.},
  month      = {10},
  articleno  = {62},
  numpages   = {39},
  keywords   = {Image captioning, deep learning, text generation}
}

@article{ANTOL2015,
  author     = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Winson Han and Devi Parikh and Dhruv Batra},
  title      = {VQA: Visual Question Answering},
  journal    = {CoRR},
  volume     = {abs/1505.00468},
  year       = {2015},
  url        = {http://arxiv.org/abs/1505.00468},
  eprinttype = {arXiv},
  eprint     = {1505.00468},
  timestamp  = {Mon, 23 Sep 2019 14:02:29 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/AntolALLPB15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@book{FRUTIGER1989,
  author    = {Adrian Frutiger},
  title     = {Signs and Symbols: Their Design and Meaning},
  publisher = {Van Nostrand Reinhold},
  year      = {1989}
}

% Beachten Sie, dass für "Grid Systems in Graphic Design" und "Logo Design Love"
% keine direkten DBLP- oder arXiv-Einträge existieren.
% Hier sind manuelle BibTeX-Einträge:
@book{MUELLERBROCKMANN1988,
  author    = {Josef M{\"{u}}ller-Brockmann},
  title     = {Grid Systems in Graphic Design / Raster Systeme F{\"{u}}r Die Visuelle Gestaltung},
  publisher = {Verlag Niggli},
  year      = {1988}
}

@book{ERHART2010,
  author    = {David Airey},
  title     = {Logo Design Love: A Guide to Creating Iconic Brand Identities},
  publisher = {Peachpit Press},
  year      = {2010}
}

@book{Lidwell2010,
  author    = {Lidwell, William and Holden, Kritina and Butler, Jill},
  title     = {Universal Principles of Design},
  publisher = {Rockport Publishers},
  year      = {2010},
  edition   = {Revised and Updated},
  abstract  = {Ein umfassendes Nachschlagewerk, das 125 Designprinzipien aus verschiedenen Disziplinen erklärt. Es behandelt Konzepte der Wahrnehmung, Kognition und sozialen Interaktion, die für effektives Design relevant sind, wie z. B. das Gesetz der Prägnanz und die 80/20-Regel.}
}

@book{Wheeler2017,
  author    = {Wheeler, Alina},
  title     = {Designing Brand Identity: An Essential Guide for the Whole Branding Team},
  publisher = {John Wiley \& Sons},
  year      = {2009},
  edition   = {3th},
  abstract  = {Ein Standardwerk für den gesamten Branding-Prozess. Wheeler beschreibt einen fünfstufigen Prozess von der Forschung über die Strategieentwicklung bis zur Implementierung der Markenidentität. Das Buch betont die Bedeutung von Klarheit, Konsistenz und einer starken visuellen Sprache für den Markenerfolg.}
}

@book{Lupton2010,
  author    = {Lupton, Ellen},
  title     = {Thinking with Type: A Critical Guide for Designers, Writers, Editors, \& Students},
  publisher = {Princeton Architectural Press},
  year      = {2010},
  edition   = {2nd},
  abstract  = {Ein grundlegendes Werk zur Typografie, das sich mit Buchstaben, Text und Rastern beschäftigt. Lupton erklärt die theoretischen Grundlagen und praktischen Regeln für den Einsatz von Schrift in der visuellen Kommunikation und betont die Rolle der Typografie für Lesbarkeit und Ausdruck.}
}

@online{pytorch_adamw,
  author  = {{PyTorch Core Team}},
  title   = {torch.optim.AdamW},
  year    = {2024},
  url     = {https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html},
  urldate = {2025-10-22}
}

@online{dvc,
  author  = {{Iterative.ai}},
  title   = {DVC - Data Version Control},
  year    = {2024},
  url     = {https://dvc.org/},
  urldate = {2025-10-22}
}

% parameter-efficient fine-tuning for foundation models
@article{zhang2025parameter,
  title         = {Parameter-Efficient Fine-Tuning for Foundation Models},
  author        = {Zhang, Dan and Feng, Tao and Xue, Lilong and Wang, Yuandong and Dong, Yuxiao and Tang, Jie},
  eprint        = {2501.13787v1},
  archiveprefix = {arXiv},
  year          = {2025}, 
  url           = {https://Awesome-PEFT-for-Foundation-Models.github.io}
}
% survey paper, geht mehr auf nachteile ohne peft ein
@article{Zhihao_2025,
title={A Comprehensive Survey of Parameter-Efficient Fine-Tuning for Large Language and Vision Models},
url={http://dx.doi.org/10.36227/techrxiv.175303890.08902658/v1},
DOI={10.36227/techrxiv.175303890.08902658/v1},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={Zhihao, Cheng and Zhihao, Shufen and Li, Ming and Jiahao, Wang and Yifan, Zhang and Xinyi, Liu and Wei, Chen and Qian, Sun},
year={2025},
month={7} }

% wasserstein gan: nachteile
@article{arjovsky2017wasserstein,
  title         = {Wasserstein GAN},
  author        = {Arjovsky, Martin and Chintala, Soumith and Bottou, Leon},
  eprint        = {1701.07875},
  archiveprefix = {arXiv},
  year          = {2017}
}

@inproceedings{ronneberger2015u,
  title        = {U-net: Convolutional networks for biomedical image segmentation},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle    = {International Conference on Medical image computing and computer-assisted intervention},
  pages        = {234--241},
  year         = {2015},
  organization = {Springer, Cham}
}

@inproceedings{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alex},
  journal={Advances in neural information processing systems},
  eprint={2105.05233},
  archivePrefix={arXiv},
  year={2021},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  eprint={2103.00020},
  archivePrefix={arXiv},
  year={2021},
}




